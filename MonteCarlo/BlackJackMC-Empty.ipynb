{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Black Jack with Monte Carlo\n",
    "In this task you are asked to find an optimal policy for a Black Jack game. You are going to use an OpenAI Gym [Black Jack environment](https://gym.openai.com/envs/Blackjack-v0/) in this task. [OpenAI Gym](https://gym.openai.com/) is a toolkit for developing and comparing reinforcement learning algorithms. One of its features is to provide various RL-ready environments to facilitate studing and developing new Reinforcement Learning algorithms.\n",
    "\n",
    "The main purposes of this notebook are to introduce:\n",
    "- OpenAI Gym environments\n",
    "- Monte Calro Methods\n",
    "- the `Exploring starts` exploration algorithm \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gym\n",
    "#!pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Black Jack Environment\n",
    "- `states` - are provided as tuples (`score`, `dealer score`, `useable ace`)\n",
    "    - `score` - the summary score of your cards (4-21)\n",
    "    - `dealer score` - the first dealer card (1-10)\n",
    "    - `useable ace` - True/False, points out if you have a useable ace\n",
    "- `actions`\n",
    "    - `0` - draw\n",
    "    - `1` - hit\n",
    "- `rewards`\n",
    "    - `1` - you won a game\n",
    "    - `0` - draw\n",
    "    - `-1` - you lost a game\n",
    "    \n",
    "    \n",
    "Let's create an environment and see how to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial State: (15, 5, False).\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Blackjack-v1')\n",
    "state, info = env.reset()\n",
    "print(f'Initial State: {state}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: (16, 7, False), Action: 1, Next state: (24, 7, False), Reward: -1.0, Terminal: True\n",
      "Terminal state: (24, 7, False), Final reward: -1.0\n"
     ]
    }
   ],
   "source": [
    "def sample_policy(state):\n",
    "    score, dealer_score, usable_ace = state\n",
    "    return 0 if score >= 19 else 1\n",
    "\n",
    "state, info = env.reset()\n",
    "terminal = False\n",
    "while not terminal:\n",
    "    action = sample_policy(state)\n",
    "    \n",
    "    next_state, reward, terminal, truncated, info = env.step(action)\n",
    "    print (f'State: {state}, Action: {action}, Next state: {next_state}, Reward: {reward}, Terminal: {terminal}')\n",
    "    \n",
    "    state = next_state\n",
    "    \n",
    "print (f'Terminal state: {state}, Final reward: {reward}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "Fill in the placeholders to complete the `update_qest` method.  You are supposed\n",
    "to compute an updated state-action value according to the formula:\n",
    "\n",
    "\n",
    "$q_{n+1} = q_{n} + \\frac{1}{n}(G_n-q_n)$\n",
    "\n",
    "where:\n",
    "- $q_{n}$ - current estimated state-action value \n",
    "- $q_{n+1}$ - new estimated state-action value \n",
    "- $G_n$ - return obtained for the explored state and action\n",
    "- $n$ - number of actions (computed separately for each action type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy:\n",
    "    \"\"\"\n",
    "    This class is used to learn and maintain the policy. It is being done by\n",
    "    learning action-value methods. \n",
    "    Properties: \n",
    "        q_est - a dictionary that stores estimated state-action values for \n",
    "                the states\n",
    "                { state : [value for action0, value for action1] }\n",
    "        n     - a dictionary that stores how many times each state-action \n",
    "                value was updated\n",
    "                { state : [no. updates for action0, no. updates for action 1]}\n",
    "    Methods:\n",
    "        act         - returns a greedy action according to a current policy.\n",
    "                      The initial policy assumes to hit until the score >= 19.\n",
    "                      Then it is gradually updated in a learning process\n",
    "        update_qest - updates a specific state-action value using a given \n",
    "                      return\n",
    "        plot        - visualizes the policy.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        # Dictionary with { state : [state action value for 0, state action value for 1]  }\n",
    "        self.q_est = {}        \n",
    "        self.n = {}\n",
    "        \n",
    "    def __initialize_state(self, state):\n",
    "        state_tuple = tuple(state.items())\n",
    "        if state_tuple in self.q_est:\n",
    "            return\n",
    "\n",
    "        score = state['player'][0]\n",
    "        should_hit = int (score < 19)\n",
    "        self.q_est[state_tuple] = [1 - should_hit, should_hit]\n",
    "        self.n[state_tuple] = [0, 0]\n",
    "        \n",
    "        \n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "        Returns a greedy action according to the current policy\n",
    "        Arguments:\n",
    "            state - a state obtained from OpenAI Gym Black Jack environment\n",
    "        Returns:\n",
    "            action - 0 for 'draw', 1 for 'hit'\n",
    "        \"\"\"\n",
    "        if isinstance(state, dict):\n",
    "            state_tuple = tuple(state.items())\n",
    "        else:\n",
    "            state_tuple = state\n",
    "        self.__initialize_state(state_tuple)\n",
    "        return np.argmax(self.q_est[state_tuple])\n",
    "                \n",
    "    def update_qest(self, state, action, g):\n",
    "        \"\"\"\n",
    "        Updates state-action value for a specific state and a specific action.\n",
    "        State-action values are computed as a mean of all returns\n",
    "        Arguments:\n",
    "            state  - a state obtained from OpenAI Gym Black Jack environment\n",
    "            action - 0 for 'draw', 1 for 'hit'\n",
    "            g      - return that should be used for updating        \n",
    "        \"\"\"\n",
    "        state_tuple = tuple(state.items())\n",
    "        self.__initialize_state(state)\n",
    "        \n",
    "        n_sa = self.n[state_tuple][action]\n",
    "        q_sa = self.q_est[state_tuple][action]\n",
    "        # ENTER YOUR CODE HERE. \n",
    "        # Update self.n[state][action] and self.q_est[state][action]\n",
    "        n_new = n_sa + 1\n",
    "        q_new = q_sa + (1/n_new)*(g - q_sa)\n",
    "    \n",
    "        self.n[state][action] = n_new\n",
    "        self.q_est[state][action] = q_new\n",
    "        \n",
    "    def plot(self, useable_ace = True):\n",
    "        \"\"\"\n",
    "        Plots a visualization of current policy. It plots the policy only for the explored\n",
    "        states. The states that haven't been explored yet, are plotted as 'unknown'\n",
    "        Arguments:\n",
    "            useable_ace - True / False. It plots different policies, whether the player\n",
    "                          has or does not have a useable ace.        \n",
    "        \"\"\"\n",
    "        states = [x for x in policy.q_est.keys() if x[2] == useable_ace]\n",
    "        rows = max(x[0] for x in states)\n",
    "        cols = max(x[1] for x in states)\n",
    "        \n",
    "        res = -1 * np.ones((rows, cols))\n",
    "        for state in states:\n",
    "            res[rows-state[0], state[1]-1] = self.act(state)\n",
    "            \n",
    "        fig = plt.figure(figsize = (12, 8))\n",
    "\n",
    "        cmap = matplotlib.colors.ListedColormap(('white', 'r', 'g'), name = 'My Cmap')\n",
    "\n",
    "        ax = sns.heatmap(res, linewidth=0.5, cmap = cmap)\n",
    "        cbar = ax.collections[0].colorbar\n",
    "        cbar.set_ticks([1, 0, -1])\n",
    "        cbar.set_ticklabels(['hit', 'draw', 'unknown'])\n",
    "        ax.set_xticks(np.arange(10) + 1)\n",
    "        ticks = np.arange(10)\n",
    "        xticklabels = [f'       {x}' for x in list(ticks+1)]\n",
    "\n",
    "        plt.xticks(ticks, xticklabels, ha = 'left')\n",
    "        ticks = np.arange(21)\n",
    "        yticklabels = [f'{x} ' + (' ' if x < 10 else '') for x in list(ticks+1)[::-1]]\n",
    "        plt.yticks(ticks, yticklabels, rotation=90, va='top')\n",
    "        plt.title('Black Jack policy with' + ('' if useable_ace else 'out') + ' a useable ace')\n",
    "\n",
    "        plt.show()        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = Policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "Complete the generate_episode function by filling in the code placeholders\n",
    "You are supposed to:\n",
    "- randomly select the initial action and execute this action \n",
    "- select every other action according to the policy and execute it\n",
    "- update policy for each state-action pair generated during the episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(policy):\n",
    "    \"\"\"\n",
    "    Generates one episode, and updates state-action values of the policy\n",
    "    acording to the provided policy.\n",
    "    \"\"\"\n",
    "    states = []\n",
    "    actions = []\n",
    "    \n",
    "    state = env.reset()\n",
    "    \n",
    "    # Choose the first action randomly\n",
    "    action = np.random.choice([0, 1])\n",
    "    \n",
    "    states.append((state, action))\n",
    "    actions.append(action)\n",
    "    \n",
    "    # ENTER YOUR CODE HERE\n",
    "    # Execute the action you chose above\n",
    "    state, reward, terminal, truncated, info = env.step(action)\n",
    "    # END OF YOUR CODE\n",
    "    \n",
    "    while not terminal:\n",
    "        # ENTER YOUR CODE HERE\n",
    "        # Choose an action according to the current policy\n",
    "        action = policy.act(state)\n",
    "        # END OF YOUR CODE\n",
    "        \n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "                \n",
    "        # ENTER YOUR CODE HERE\n",
    "        # Execute the action you chose above\n",
    "        state, reward, terminal, truncated, info = env.step(action)\n",
    "        # END OF YOUR CODE\n",
    "        \n",
    "    for state, action in zip(states, actions):\n",
    "        # ENTER YOUR CODE HERE\n",
    "        # Update qest\n",
    "        policy.update_qest(state, action, reward)\n",
    "        pass \n",
    "    \n",
    "        # END OF YOUR CODE\n",
    "        \n",
    "        policy.update_qest(state, action, reward)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "Generate episodes and learn from them until you learn the optimal policy. You may use `policy.plot()` to visualize your policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\Desktop\\Studia\\Informatyka - II stopień\\II semestr\\Reinforcement Learning\\Reinforcement-Learning\\MonteCarlo\\BlackJackMC-Empty.ipynb Cell 13\u001b[0m in \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Desktop/Studia/Informatyka%20-%20II%20stopie%C5%84/II%20semestr/Reinforcement%20Learning/Reinforcement-Learning/MonteCarlo/BlackJackMC-Empty.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Generate enough episodes to obtain the optimal policy\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Desktop/Studia/Informatyka%20-%20II%20stopie%C5%84/II%20semestr/Reinforcement%20Learning/Reinforcement-Learning/MonteCarlo/BlackJackMC-Empty.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m (\u001b[39m1000000\u001b[39m)):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Desktop/Studia/Informatyka%20-%20II%20stopie%C5%84/II%20semestr/Reinforcement%20Learning/Reinforcement-Learning/MonteCarlo/BlackJackMC-Empty.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     generate_episode(policy)\n",
      "\u001b[1;32md:\\Desktop\\Studia\\Informatyka - II stopień\\II semestr\\Reinforcement Learning\\Reinforcement-Learning\\MonteCarlo\\BlackJackMC-Empty.ipynb Cell 13\u001b[0m in \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Desktop/Studia/Informatyka%20-%20II%20stopie%C5%84/II%20semestr/Reinforcement%20Learning/Reinforcement-Learning/MonteCarlo/BlackJackMC-Empty.ipynb#X15sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m state \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mreset()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Desktop/Studia/Informatyka%20-%20II%20stopie%C5%84/II%20semestr/Reinforcement%20Learning/Reinforcement-Learning/MonteCarlo/BlackJackMC-Empty.ipynb#X15sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m terminal:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Desktop/Studia/Informatyka%20-%20II%20stopie%C5%84/II%20semestr/Reinforcement%20Learning/Reinforcement-Learning/MonteCarlo/BlackJackMC-Empty.ipynb#X15sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     action \u001b[39m=\u001b[39m policy\u001b[39m.\u001b[39;49mact(state)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Desktop/Studia/Informatyka%20-%20II%20stopie%C5%84/II%20semestr/Reinforcement%20Learning/Reinforcement-Learning/MonteCarlo/BlackJackMC-Empty.ipynb#X15sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     next_state, reward, terminal, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Desktop/Studia/Informatyka%20-%20II%20stopie%C5%84/II%20semestr/Reinforcement%20Learning/Reinforcement-Learning/MonteCarlo/BlackJackMC-Empty.ipynb#X15sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     states\u001b[39m.\u001b[39mappend(state)\n",
      "\u001b[1;32md:\\Desktop\\Studia\\Informatyka - II stopień\\II semestr\\Reinforcement Learning\\Reinforcement-Learning\\MonteCarlo\\BlackJackMC-Empty.ipynb Cell 13\u001b[0m in \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Desktop/Studia/Informatyka%20-%20II%20stopie%C5%84/II%20semestr/Reinforcement%20Learning/Reinforcement-Learning/MonteCarlo/BlackJackMC-Empty.ipynb#X15sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Desktop/Studia/Informatyka%20-%20II%20stopie%C5%84/II%20semestr/Reinforcement%20Learning/Reinforcement-Learning/MonteCarlo/BlackJackMC-Empty.ipynb#X15sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     state_tuple \u001b[39m=\u001b[39m state\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Desktop/Studia/Informatyka%20-%20II%20stopie%C5%84/II%20semestr/Reinforcement%20Learning/Reinforcement-Learning/MonteCarlo/BlackJackMC-Empty.ipynb#X15sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__initialize_state(state_tuple)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Desktop/Studia/Informatyka%20-%20II%20stopie%C5%84/II%20semestr/Reinforcement%20Learning/Reinforcement-Learning/MonteCarlo/BlackJackMC-Empty.ipynb#X15sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39margmax(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_est[state_tuple])\n",
      "\u001b[1;32md:\\Desktop\\Studia\\Informatyka - II stopień\\II semestr\\Reinforcement Learning\\Reinforcement-Learning\\MonteCarlo\\BlackJackMC-Empty.ipynb Cell 13\u001b[0m in \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Desktop/Studia/Informatyka%20-%20II%20stopie%C5%84/II%20semestr/Reinforcement%20Learning/Reinforcement-Learning/MonteCarlo/BlackJackMC-Empty.ipynb#X15sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__initialize_state\u001b[39m(\u001b[39mself\u001b[39m, state):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Desktop/Studia/Informatyka%20-%20II%20stopie%C5%84/II%20semestr/Reinforcement%20Learning/Reinforcement-Learning/MonteCarlo/BlackJackMC-Empty.ipynb#X15sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     state_tuple \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(state\u001b[39m.\u001b[39;49mitems())\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Desktop/Studia/Informatyka%20-%20II%20stopie%C5%84/II%20semestr/Reinforcement%20Learning/Reinforcement-Learning/MonteCarlo/BlackJackMC-Empty.ipynb#X15sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     \u001b[39mif\u001b[39;00m state_tuple \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_est:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Desktop/Studia/Informatyka%20-%20II%20stopie%C5%84/II%20semestr/Reinforcement%20Learning/Reinforcement-Learning/MonteCarlo/BlackJackMC-Empty.ipynb#X15sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "# ENTER YOUR CODE HERE\n",
    "env = gym.make('Blackjack-v1')\n",
    "policy = Policy()\n",
    "# Generate enough episodes to obtain the optimal policy\n",
    "for i in tqdm(range (1000000)):\n",
    "    generate_episode(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.plot(False)\n",
    "policy.plot(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
